State University of New York College at Old Westbury


                    Student Evaluations Committee Report


                                June 29, 2011

 I.   Background


     Academic Affairs arranged a pilot test of “CoursEval,” an online system
     of conducting student evaluations of teaching.  The Psychology
     Department and Academic Affairs agreed that the test would be conducted
     in psychology courses only, and that Academic Affairs would manage the
     new system if the pilot was successful and the system was adopted on a
     college-wide basis.


     In connection with the pilot, the Provost asked the Faculty Senate
     Chair to assemble an ad hoc committee of faculty to examine the
     existing student evaluation survey, and recommend changes that might
     enhance the degree to which it provides informative and actionable
     instructor feedback.


     This report summarizes the work of the committee, the results of the
     pilot, and the Psychology Department’s conclusions and recommendations
     for next steps.


 II.  The Work of the Committee


      The Committee developed a first draft of the new survey shortly after
     its first meeting on June 7, 2010.   Further development halted over
     the summer, and the task was taken up again on September 8th.  Program
     Chairs were given copies of a draft of the survey, a background
     document, and a written feedback form at the September 23rd Provost’s
     Council meeting for distribution to their respective faculty.  On
     October 28, 2010, a “final” draft (#3) was sent via email to the entire
     faculty with a request for specific comments and general feedback.
     This draft was also sent to the SGA President as the route by which the
     committee sought student input.


      The Chair revised the draft based on the comments received from
     faculty and students, and asked for the committee’s approval to send a
     final version to the vendor for programming.  Members of the committee
     from the School of Education, as representatives of the SOE faculty,
     raised an objection to the idea of revising the existing evaluation
     questions.  The committee met with the Provost on November 12th in an
     effort to resolve the impasse, and was instructed by the Provost to
     move ahead with the pilot.  The final version of the draft survey was
     sent to the vendor on November 19, 2010.

 III. The Results of the Pilot

     A.     Set-Up and Execution

         • The Chair of the Committee served as the administrator of the
           online course evaluation system.  The administrator programs the
           survey questions into the system; coordinates with Computing
           Services in setting up the list of courses, faculty, and
           students; and generates the evaluation reports.


         • With excellent customer service and technical support from the
           vendor, set-up and execution went smoothly.  Programming the
           survey required about four hours of work by the administrator,
           and two weeks of work on the part of Computing Services
           personnel.




         • The survey was available for 21 days, from November 30th to
           December 21st.  During this time, students  received e-mail
           alerts every two days encouraging them to access the survey and
           fill it out.  Faculty members received bi-weekly email reminders
           that included a count of how many students had completed the
           survey (with no identifying information as to who had done so).


         • The only problem encountered was a “bounce-back” to the
           administrator’s email address of about 20 student email alerts
           (every two days) that could not be delivered to students who do
           not use their Old Westbury email accounts.


     B.     Completion Rates and Sample Composition


         • Thirty-eight (38) undergraduate course sections were assessed.
           A total of 179 completed surveys were received – 19% of the
           1,074 that were expected.  These 179 completed surveys were
           generated by 91 students (because students who were taking more
           than one course filled out more than one survey).


         • The number of completed surveys within individual courses ranged
           from 1 to 14, with a mean of 4.71 (SD = 3.17) and a median of 4.
            The percentage of students completing the survey in each course
           ranged from 3% to 62%, with a mean of 18.24% (SD = 13.28) and a
           median of 17%.


         • The mean cumulative GPA of the students who completed the survey
           (based on a random sample of 50 students) was 3.20 (SD = .55),
           which is significantly higher than the mean GPA of 2.86 for all
           psychology majors enrolled in the Fall 2010 semester[1].


IV.   Conclusions and Recommendations


     A.     Conclusions


         • The CoursEval product appears to provide a user-friendly and
           relatively trouble-free way to conduct student evaluations
           online.  However, the low response rates and apparent sampling
           bias resulting in an over-representation of academically
           stronger students are of serious concern.


         • Also of concern is the validity of the evaluation instrument
           itself.  Although the Committee appears to have taken care to
           base the survey questions on an empirical framework with
           demonstrated reliability, there is no evidence that the survey
           as developed is reliable, or that it provides a valid assessment
           of teaching effectiveness.  As such, its use in helping
           instructors improve their teaching skills or serving as input
           into decisions on reappointment, promotion, and tenure, is
           highly questionable without further analysis of the evaluation
           data from the pilot.


         • Unfortunately, given the low response rate, the quantity of
           available data is not sufficient to conduct such an analysis.  A
           second, college-wide pilot would provide enough data for this
           purpose; however, the matter of low response rates and
           potentially biased sampling remains.


     B.     Recommendations


        In light of the concerns noted above, and because Academic Affairs
        is responsible for the assessment of the academic program, it is
        incumbent upon Academic Affairs to provide sufficient resources and
        time to establish a committee of interested faculty who will be
        able to give serious consideration to the issues raised by the
        pilot, including, but not limited to (a) the usefulness and purpose
        of student evaluations of teaching, in general, and (b) the
        challenge of substantially raising response rates, particularly if
        the use of an online system is inevitable.


        In the present case, the committee had neither the time nor the
        support to be able to develop the evaluation instrument in a
        rigorous manner.  For example, with the committee convened in June,
        and the final draft of the instrument due by early November, it was
        not possible to do a thorough search of the relevant literature to
        guide and undergird the development process.  Further, because much
        of the committee’s work took place between June 15th and August
        15th, when faculty typically are not on campus, it was often
        necessary to conduct committee meetings without all members present
        (which slowed the rate of progress during this time).  Once the
        semester began, the rate of progress was limited by the need to
        allow enough time to circulate the draft among students and faculty
        to obtain their feedback.  After the draft was approved, and before
        the survey could be made available to students, additional time was
        needed so that the committee chair could learn how to use the
        online program, and so that any “glitches” in the system could be
        worked out.


        Thus, if it is the goal of Academic Affairs to revise the current
        student evaluation survey and move toward the implementation of an
        online method of data collection, it will be necessary to approach
        the tasks of survey development and pilot testing with a level of
        commitment that is commensurate with the importance of the course
        evaluation process.


        Respectfully submitted,


        K. Greenberg
        6/29/11
-----------------------
      [1] t(49) = 4.409, p = .000